---

title: Introduction to Statistical Learning

author: Dr. Lasanthi Watagoda

date: January 19, 2021

output:

  prettydoc::html_pretty:

    toc: true

    theme: cayman

    highlight: github

    math: katex

    number_sections: true

---



```{r setup, include=FALSE}

knitr::opts_chunk$set(echo = TRUE)

library(ggplot2)

library(ISLR)

```
## 1 What is Statistical learning?
  Statistical learning refers to a vast set of tools for understanding data.

These tools can be classified as:
  1. Supervised
  2. Unsupervised
  
## 1.1 Supervised Learning
This involves building a statistical model for predicting, or estimating, an output based on one or more inputs.
Problems of this nature occur in fields as diverse as business, medicine, astrophysics, and public policy.

## 1.2 Unsupervised Learning
Here, there are inputs but no supervising outputs; nevertheless we can learn relationships and structure from such data.

## 1.3 Data sets
To provide an illustration of some applications of statistical learning, we briefly discuss three real-world data sets.

Wage data — Wage
Stock Market data — Smarket
Gene Expression data — NCI60
## 1.3.1 Wage data — Wage
In this application we examine a number of factors that relate to wages for a group of males from the Atlantic region of the United States. In particular, we wish to understand the association between an employee’s age and education, as well as the calendar year, on his wage.
```{r}

data(Wage)
head(Wage)
nrow(Wage)
dim(Wage)

##        year age           maritl     race       education             region
## 231655 2006  18 1. Never Married 1. White    1. < HS Grad 2. Middle Atlantic
## 86582  2004  24 1. Never Married 1. White 4. College Grad 2. Middle Atlantic
## 161300 2003  45       2. Married 1. White 3. Some College 2. Middle Atlantic
## 155159 2003  43       2. Married 3. Asian 4. College Grad 2. Middle Atlantic
## 11443  2005  50      4. Divorced 1. White      2. HS Grad 2. Middle Atlantic
## 376662 2008  54       2. Married 1. White 4. College Grad 2. Middle Atlantic
##              jobclass         health health_ins  logwage      wage
## 231655  1. Industrial      1. <=Good      2. No 4.318063  75.04315
## 86582  2. Information 2. >=Very Good      2. No 4.255273  70.47602
## 161300  1. Industrial      1. <=Good     1. Yes 4.875061 130.98218
## 155159 2. Information 2. >=Very Good     1. Yes 5.041393 154.68529
## 11443  2. Information      1. <=Good     1. Yes 4.318063  75.04315
## 376662 2. Information 2. >=Very Good     1. Yes 4.845098 127.11574
```



Consider, for example, wage versus age for each of the individuals in the data set.

Create a scatter plot (shown here) for wage versus age
```{r}
ggplot(data = Wage, aes(x=age,y=wage)) + geom_point(color = "steelblue") + geom_smooth(se = FALSE) + theme_bw()
```




Describe the plot you created in i)
There is evidence that wage increases with age but then decreases again after approximately age 60

Create a scatter plot (shown here) for wage versus year
```{r}
ggplot(data = Wage, aes(x=year,y=wage)) + geom_point(color = "steelblue") + geom_smooth(se = FALSE) + theme_bw() 
```



Describe the plot you created in iii)
There is a slow but steady increase of approximately $10,000 in the average wage between 2003 and 2009

Create a boxplot (shown here) for wage for each education level.
```{r}
ggplot(data = Wage, aes(x=education,y=wage,fill=education)) + geom_boxplot(color = "purple") + geom_smooth(se = FALSE) + theme_bw() + labs(title = "Wage According to Education", x = "Education Level", y = "Wage")
```

Describe the plot you created in v)

  This plot shows the 1st 2nd 3rd and 4th quartiles of the wages per education level. The trend it shows is that wages increase with education level.
  
Note:
  
  The Wage data involves predicting a continuous or quantitative output value. 

This is often referred to as a regression problem.

### Stock Market data — Smarket

In this case we instead wish to predict a non-numerical value—that is, a categorical output.

```{r}
data("Smarket")
head(Smarket)
#dim(Smarket)
#head(Smarket)

##   Year   Lag1   Lag2   Lag3   Lag4   Lag5 Volume  Today Direction
## 1 2001  0.381 -0.192 -2.624 -1.055  5.010 1.1913  0.959        Up
## 2 2001  0.959  0.381 -0.192 -2.624 -1.055 1.2965  1.032        Up
## 3 2001  1.032  0.959  0.381 -0.192 -2.624 1.4112 -0.623      Down
## 4 2001 -0.623  1.032  0.959  0.381 -0.192 1.2760  0.614        Up
## 5 2001  0.614 -0.623  1.032  0.959  0.381 1.2057  0.213        Up
## 6 2001  0.213  0.614 -0.623  1.032  0.959 1.3491  1.392        Up
```


The goal is to predict whether the index will increase or decrease on a given day using the past 5 days’ percentage changes in the index.

Here the statistical learning problem does not involve predicting a numerical value. Instead it involves predicting whether a given day’s stock market performance will fall into the up bucket or the down bucket.

Note:
  
  This is known as a classification problem.
Create a boxplot (shown here) for yesterday’s percentage change with the Direction variable
```{r}
p1 <- ggplot(data = Smarket, aes(x = Today, y = Lag1, fill = Direction)) + geom_boxplot() 
p2 <- ggplot(data = Smarket, aes(x = Today, y = Lag2, fill = Direction)) + geom_boxplot()
p3 <- ggplot(data = Smarket, aes(x = Today, y = Lag3, fill = Direction)) + geom_boxplot()

library(cowplot)
plot_grid(p1,p2,p3)
```


Is there any indication that there is an association between the past and present performance of the stock market?

Every plot looks almost identical, which suggests that there is no simple strategy for using the previous three day's results to predict today's return.

### Gene Expression data — NCI60

The previous two applications illustrate data sets with both input and output variables. However, another important class of problems involves situations in which we only observe input variables, with no corresponding output.

Example: In a marketing setting, we might have demographic information for a number of current customers. We may wish to understand which types of customers are similar to each other by grouping individuals according to their observed characteristics.

Note:
  
  This is known as a clustering problem.
We consider the NCI60 data set, which consists of 6830 gene expression measurements for each of 64 cancer cell lines. Instead of predicting a particular output variable, we are interested in determining whether there are groups, or clusters, among the cell lines based on their gene expression measurements. This is a difficult question to address, in part because there are thousands of gene expression measurements per cell line, making it hard to visualize the data.

# What do we cover in this class?

  * In Chapter 2 we introduce the basic terminology and concepts behind statistical learning. This chapter also presents the KK-nearest neighbor classifier, a very simple method that works surprisingly well on many problems.

* Chapter 3 reviews linear regression, the fundamental starting point for all regression methods.

* A central problem in all statistical learning situations involves choosing the best method for a given application. Hence, in Chapter 5 we introduce cross-validation and the bootstrap, which can be used to estimate the accuracy of a number of different methods in order to choose the best one.

* Chapter 6 we consider a host of linear methods, both classical and more modern, which offer potential improvements over standard linear regression. These include stepwise selection, ridge regression, principal components regression, partial least squares, and the lasso.